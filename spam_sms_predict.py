# -*- coding: utf-8 -*-
"""spam_sms_predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-GMm6I5KaGPDGqgCCU5kWK44pkp1FHBf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("spam.csv", encoding='latin-1')

df.head()
df.tail()

df.isnull().sum()
df.shape

print(df.columns)

df.isnull().sum()

df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

df.rename(columns={'v1':'target','v2':'text'},inplace=True)
df.head()

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()

df['target']=encoder.fit_transform(df['target'])

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(keep='first',inplace=True)

df.shape

df['target'].value_counts()

import seaborn as sns

sns.barplot(df['target'].value_counts())

import nltk

df['num_char']=df['text'].apply(len)

nltk.download('punkt_tab')
df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

df['num_sent']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df[['num_char','num_words','num_sent']].describe()

df[df['target']==0][['num_char','num_words','num_sent']].describe()

df[df['target']==1][['num_char','num_words','num_sent']].describe()

sns.histplot(df[df['target']==0]['num_char'])
sns.histplot(df[df['target']==1]['num_char'],color='green')

sns.pairplot(df,hue='target')

import nltk
nltk.download('stopwords')
import string
from nltk.stem import PorterStemmer # Import the PorterStemmer

ps = PorterStemmer()
def transform_text(text):
  text=text.lower()
  text=nltk.word_tokenize(text)

  y=[]
  for i in text:
    if i.isalnum():
      y.append(i)

      text=y[:]
  y.clear()

  for i in text:
    if i not in nltk.corpus.stopwords.words('english') and i not in string.punctuation:
      y.append(i)
  text=y[:]
  y.clear()
  for i in text:
    y.append(ps.stem(i))
  return ' '.join(y)

transform_text('hi ,how are you neha?')

df['text'][10]

ps.stem('loving')

df['transformed_text']=df['text'].apply(transform_text)

df.head()

from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')

# Now you can generate the word cloud
spam_wc =  wc.generate(df[df["target"] == 1]['transformed_text'].str.cat(sep=''))

plt.figure(figsize=(12,6))
plt.imshow(spam_wc)

ham_wc =  wc.generate(df[df["target"] == 0]['transformed_text'].str.cat(sep=''))

plt.figure(figsize=(12,6))
plt.imshow(ham_wc)

df.info()

spam_corpus=[]
for msg in df[df['target'] ==1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(38))[0])
plt.xticks(rotation='vertical')
plt.show()

ham_corpus=[]
for msg in df[df['target'] ==0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)

len(ham_corpus)

from collections import Counter
sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(38))[0] )
plt.xticks(rotation='vertical')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv=CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)

X=tfidf.fit_transform(df['transformed_text']).toarray()

# from sklearn.preprocessing import MinMaxScaler
# scaler=MinMaxScaler()
# X =scaler.fit_transform(X)

# X=np.hstack((X,df['num_char'].values.reshape(-1,1)))

X.shape

y=df['target'].values

y

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

gnb.fit(X_train,y_train)
y_pred1=gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(X_train,y_train)
y_pred2=mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

bnb.fit(X_train,y_train)
y_pred3=bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc=SVC(kernel='sigmoid',gamma=1.0)
knc=KNeighborsClassifier()
dtc=DecisionTreeClassifier(max_depth=5)
lrc=LogisticRegression(solver='liblinear',penalty='l1')
rfc=RandomForestClassifier(n_estimators=50,random_state=2)
abc=AdaBoostClassifier(n_estimators=50,random_state=2)
bc=BaggingClassifier(n_estimators=50,random_state=2)
etc=ExtraTreesClassifier(n_estimators=50,random_state=2)
gbdt=GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb=XGBClassifier(n_estimators=50,random_state=2)
#

clfs={
    'SVC':svc,
    'KN':knc,
    'DT':dtc,
    'LR':lrc,
    'RF':rfc,
    'AdaBoost':abc
}

def train_classifier(clf,X_train,y_train,X_test,y_test):
  clf.fit(X_train,y_train)
  y_pred=clf.predict(X_test)
  accuracy=accuracy_score(y_test,y_pred)
  precision=precision_score(y_test,y_pred)

  return accuracy,precision

train_classifier(svc,X_train,y_train,X_test,y_test)

accuracy_scores=[]
precision_scores=[]

for name,clf in clfs.items():
  current_accuracy,current_precision=train_classifier(clf,X_train,y_train,X_test,y_test)

  print("For ",name)
  print("Accuracy - ",current_accuracy)
  print("Precision - ",current_precision)

  accuracy_scores.append(current_accuracy)
  precision_scores.append(current_precision)

performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)

performance_df

performance_df1 = pd.melt(performance_df, id_vars = "Algorithm", var_name = "variable", value_name = "value")
sns.catplot(x='Algorithm',y='value',hue='variable', data=performance_df1,kind='bar',height=5)
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()

temp_df=pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)

new_df=performance_df.merge(temp_df,on='Algorithm')

new_df_scaled = new_df.merge(temp_df,on='Algorithm')

new_df_scaled

svc=SVC(kernel='sigmoid',gamma=1.0)
knc=KNeighborsClassifier()
etc=ExtraTreesClassifier(n_estimators=50,random_state=2)
#
from sklearn.ensemble import VotingClassifier
voting = VotingClassifier(estimators=[('svm',svc),('knn',knc),('et',etc)],voting='hard')

voting.fit(X_train,y_train)
y_pred=voting.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))

